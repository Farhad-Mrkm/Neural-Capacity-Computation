# -*- coding: utf-8 -*-
"""Chi_Square_Optical_Intensity_Sg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AhIw3wthvSy2t2bg5DRvKTdGAD-czirZ
"""

# Commented out IPython magic to ensure Python compatibility.

# import tensorflow as tf
import os
import scipy
import h5py
import glob, os
from scipy.io import loadmat
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
from keras import regularizers
from numpy import mean
from numpy import std
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
import torch
import torch.nn as nn
from tqdm.auto import tqdm, trange
from numpy.random import default_rng
import torch.nn.functional as F

import gc
gc.collect()
#.mount('/content/drive')
print(np.version.version)

class MI_Est_Losses():
  def __init__(self, estimator, device):
    """Estimate variational lower bounds on mutual information based on
      a function T(X,Y) represented by a NN using variational lower bounds.
    Args:
      estimator: string specifying estimator, one of:
        'smile', 'nwj', 'infonce', 'tuba', 'js', 'interpolated'
      device: the device to use (CPU or GPU)
    """    
    self.device = device
    self.estimator = estimator



  def logmeanexp_diag(self, x):
    """Compute logmeanexp over the diagonal elements of x."""
    batch_size = x.size(0)

    logsumexp = torch.logsumexp(x.diag(), dim=(0,))
    num_elem = batch_size

    return logsumexp - torch.log(torch.tensor(num_elem).float()).to(self.device)


  def logmeanexp_nodiag(self, x, dim=None):
      batch_size = x.size(0)
      if dim is None:
          dim = (0, 1)

      logsumexp = torch.logsumexp(
          x - torch.diag(np.inf * torch.ones(batch_size).to(self.device)), dim=dim)

      try:
          if len(dim) == 1:
              num_elem = batch_size - 1.
          else:
              num_elem = batch_size * (batch_size - 1.)
      except ValueError:
          num_elem = batch_size - 1
      return logsumexp - torch.log(torch.tensor(num_elem)).to(self.device)


  def tuba_lower_bound(self, scores, log_baseline=None):
      if log_baseline is not None:
          scores -= log_baseline[:, None]

      # First term is an expectation over samples from the joint,
      # which are the diagonal elmements of the scores matrix.
      joint_term = scores.diag().mean()

      # Second term is an expectation over samples from the marginal,
      # which are the off-diagonal elements of the scores matrix.
      marg_term = self.logmeanexp_nodiag(scores).exp()
      return  joint_term - marg_term/(torch.tensor(2))-torch.log(torch.tensor(2))+1


  def nwj_lower_bound(self, scores):
      return self.tuba_lower_bound(scores - 1.)


  def infonce_lower_bound(scores):
      nll = scores.diag().mean() - scores.logsumexp(dim=1)
      # Alternative implementation:
      # nll = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=tf.range(batch_size))
      mi = torch.tensor(scores.size(0)).float().log() + nll
      mi = mi.mean()
      return mi


  def js_fgan_lower_bound(self, f):
      """Lower bound on Jensen-Shannon divergence from Nowozin et al. (2016)."""
      f_diag = f.diag()
      first_term = -F.softplus(-f_diag).mean()
      n = f.size(0)
      second_term = (torch.sum(F.softplus(f)) -
                    torch.sum(F.softplus(f_diag))) / (n * (n - 1.))
      return first_term - second_term


  def js_lower_bound(self, f):
      """Obtain density ratio from JS lower bound then output MI estimate from NWJ bound."""
      nwj = self.nwj_lower_bound(f)
      js = self.js_fgan_lower_bound(f)

      with torch.no_grad():
          nwj_js = nwj - js

      return js + nwj_js


  def dv_bound(self, f):
      """
      Donsker-Varadhan lower bound, but upper bounded by using log outside.
      Similar to MINE, but did not involve the term for moving averages.
      """
      first_term = f.diag().mean()
      second_term = self.logmeanexp_nodiag(f)

      return first_term - second_term

  def new_lower_bound_dv(self, scores, log_baseline=None):
      if log_baseline is not None:
          scores -= log_baseline[:, None]

     
      joint_term = scores.diag().mean()

     
      marg_term = self.logmeanexp_nodiag(scores).exp()
      return  joint_term - marg_term/(torch.tensor(2))-torch.log(torch.tensor(2))+1
  def mine_lower_bound(self, f, buffer=None, momentum=0.9):
      """
      MINE lower bound based on DV inequality.
      """
      if buffer is None:
          buffer = torch.tensor(1.0).to(self.device)
      first_term = f.diag().mean()

      buffer_update = self.logmeanexp_nodiag(f).exp()
      with torch.no_grad():
          second_term = self.logmeanexp_nodiag(f)
          buffer_new = buffer * momentum + buffer_update * (1 - momentum)
          buffer_new = torch.clamp(buffer_new, min=1e-4)
          third_term_no_grad = buffer_update / buffer_new

      third_term_grad = buffer_update / buffer_new

      return first_term - second_term - third_term_grad + third_term_no_grad, buffer_update


  def smile_lower_bound(self,f, clip=None):
      if clip is not None:
          f_ = torch.clamp(f, -clip, clip)
      else:
          f_ = f
      z = self.logmeanexp_nodiag(f_, dim=(0, 1))
      dv = f.diag().mean() - z

      js = self.js_fgan_lower_bound(f)

      with torch.no_grad():
          dv_js = dv - js

      return js + dv_js


  def _ent_js_fgan_lower_bound(self, vec, ref_vec):
      """Lower bound on Jensen-Shannon divergence from Nowozin et al. (2016)."""
      first_term = -F.softplus(-vec).mean()
      second_term = torch.sum(F.softplus(ref_vec)) / ref_vec.size(0)
      return first_term - second_term

  def _ent_smile_lower_bound(self, vec, ref_vec, clip=None):
      if clip is not None:
          ref = torch.clamp(ref_vec, -clip, clip)
      else:
          ref = ref_vec

      batch_size = ref.size(0)
      z = log_mean_ef_ref = torch.logsumexp(ref, dim=(0, 1)) - torch.log(torch.tensor(batch_size)).to(self.device)
      dv = vec.mean() - z
      js = self._ent_js_fgan_lower_bound(vec, ref_vec)

      with torch.no_grad():
          dv_js = dv - js
      
      return js + dv_js

  def entropic_smile_lower_bound(self, f, clip=None):
      t_xy, t_xy_ref, t_x, t_x_ref, t_y, t_y_ref = f

      d_xy = self._ent_smile_lower_bound(t_xy, t_xy_ref, clip=clip)
      d_x = self._ent_smile_lower_bound(t_x, t_x_ref, clip=clip)
      d_y = self._ent_smile_lower_bound(t_y, t_y_ref, clip=clip)

      return d_xy, d_x, d_y

  def chisquare_pred(self, vec, ref_vec, beta=.9, with_smile=False, clip=None):
      b = torch.tensor(beta).to(self.device)
      if with_smile:
        if clip is not None:
            ref = torch.clamp(ref_vec, -clip, clip)
        else:
            ref = ref_vec
        
        batch_size = ref.size(0)
        z = log_mean_ef_ref = torch.logsumexp(ref, dim=(0, 1)) - torch.log(torch.tensor(batch_size)).to(self.device)
      else:
        z = log_mean_ef_ref = torch.mean(torch.exp(ref_vec))/b - torch.log(b) - 1
      
      dv = vec.mean() - z
      js = self._ent_js_fgan_lower_bound(vec, ref_vec)

      with torch.no_grad():
          dv_js = dv - js
      
      return js + dv_js

  def chi_square_lower_bound(self, f, beta=.9, with_smile=False, clip=None): 
      t_xy, t_xy_ref = f

      d_xy = self.chisquare_pred(t_xy, t_xy_ref, beta=beta, with_smile=with_smile, clip=clip)
    
      return d_xy

  def mi_est_loss(self, net_output, **kwargs):
      """Estimate variational lower bounds on mutual information.

    Args:
      net_output: output(s) of the neural network estimator

    Returns:
      scalar estimate of mutual information
      """
      if self.estimator == 'infonce':
          mi = self.infonce_lower_bound(net_output)
      elif self.estimator == 'nwj':
          mi = self.nwj_lower_bound(net_output)
      elif self.estimator == 'tuba':
          mi = self.tuba_lower_bound(net_output, **kwargs)
      elif self.estimator == 'js':
          mi = self.js_lower_bound(net_output)
      elif self.estimator =='dv_bound':
          mi=self.dv_bound(net_output)
      elif self.estimator == 'smile':
          mi = self.smile_lower_bound(net_output, **kwargs)
      elif self.estimator == 'dv':
          mi = self.dv_bound(net_output)
      elif self.estimator == 'ent_smile':
          mi = self.entropic_smile_lower_bound(net_output, **kwargs)
      elif self.estimator == 'chi_square':
          mi = self.chi_square_lower_bound(net_output, **kwargs)
      elif self.estimator =='new_dv_lower':
          mi=self.new_lower_bound_dv(net_output,**kwargs)
      return mi

def mlp(dim, hidden_dim, output_dim, layers, activation):
    """Create a mlp from the configurations."""
    activation = {
        'relu': nn.ReLU,
        'lrelu': nn.LeakyReLU
    }[activation]

    seq = [nn.Linear(dim, hidden_dim), activation()]
    for _ in range(layers):
        seq += [nn.Linear(hidden_dim, hidden_dim), activation()]
    seq += [nn.Linear(hidden_dim, output_dim)]

    return nn.Sequential(*seq)


class SeparableCritic(nn.Module):
    """Separable critic. where the output value is g(x) h(y). """

    def __init__(self, dim, hidden_dim, embed_dim, layers, activation, **extra_kwargs):
        super(SeparableCritic, self).__init__()
        self._g = mlp(dim, hidden_dim, embed_dim, layers, activation)
        self._h = mlp(dim, hidden_dim, embed_dim, layers, activation)

    def forward(self, x, y):
        scores = torch.matmul(self._h(y), self._g(x).t())
        return scores


class ConcatCritic(nn.Module):
    """Concat critic, where we concat the inputs and use one MLP to output the value."""

    def __init__(self, dim, hidden_dim, layers, activation, **extra_kwargs):
        super(ConcatCritic, self).__init__()
        # output is scalar score
        self._f = mlp(dim * 2, hidden_dim, 1, layers, activation)

    def forward(self, x, y):
        batch_size = x.size(0)
        # Tile all possible combinations of x and y
        x_tiled = torch.stack([x] * batch_size, dim=0)
        y_tiled = torch.stack([y] * batch_size, dim=1)
        # xy is [batch_size * batch_size, x_dim + y_dim]
        xy_pairs = torch.reshape(torch.cat((x_tiled, y_tiled), dim=2), [
                                 batch_size * batch_size, -1])
        # Compute scores for each x_i, y_j pair.
        scores = self._f(xy_pairs)
        return torch.reshape(scores, [batch_size, batch_size]).t()

class EntropicCritic(nn.Module):
  # Inner class that defines the neural network architecture
  def __init__(self, dim, hidden_dim, embed_dim, layers, activation):
      super(EntropicCritic, self).__init__()
      self.f = mlp(dim, hidden_dim, embed_dim, layers, activation)

  def forward(self, inp):
      output = self.f(inp)
      return output


class ConcatEntropicCritic():
    """Concat entropic critic, where we concat the inputs and use one MLP to output the value."""
    def __init__(self, dim, hidden_dim, layers, activation, 
                 device, ref_batch_factor=1,**extra_kwargs):
        # output is scalar score
        self.ref_batch_factor = ref_batch_factor
        self.fxy = EntropicCritic(dim * 2, hidden_dim, 1, layers, activation)
        self.fx = EntropicCritic(dim, hidden_dim, 1, layers, activation)
        self.fy = EntropicCritic(dim, hidden_dim, 1, layers, activation)
        self.device = device

    def _uniform_sample(self, data, batch_size):
      # Sample the reference uniform distribution
      data_min = data.min(dim=0)[0]
      data_max = data.max(dim=0)[0]
      return (data_max - data_min) * torch.rand((batch_size, data_min.shape[0])).to(self.device) + data_min

    def to(self, device):
      self.fxy.to(device)
      self.fx.to(device)
      self.fy.to(device)

    def forward(self, x, y):
        batch_size = x.size(0)
        XY = torch.cat((x, y), dim=1)
        X_ref = self._uniform_sample(x, batch_size=int(
            self.ref_batch_factor * batch_size))
        Y_ref = self._uniform_sample(y, batch_size=int(
            self.ref_batch_factor * batch_size))
        XY_ref = torch.cat((X_ref, Y_ref), dim=1)

        # Compute t function outputs approximated by NNs
        t_xy = self.fxy(XY)
        t_xy_ref = self.fxy(XY_ref)
        t_x = self.fx(x)
        t_x_ref = self.fx(X_ref)
        t_y = self.fy(y)
        t_y_ref = self.fy(Y_ref)
        return (t_xy, t_xy_ref, t_x, t_x_ref, t_y, t_y_ref)

class chiCritic():
    """Concat entropic critic, where we concat the inputs and use one MLP to output the value."""
    def __init__(self, dim, hidden_dim, layers, activation, 
                 device, ref_batch_factor=1,**extra_kwargs):
        # output is scalar score
        self.ref_batch_factor = ref_batch_factor
        self.fxy = EntropicCritic(dim * 2, hidden_dim, 1, layers, activation)
        
        
        self.device = device

    def _uniform_sample(self, data, batch_size):
      # Sample the reference uniform distribution
      data_min = data.min(dim=0)[0]
      data_max = data.max(dim=0)[0]
      return (data_max - data_min) * torch.rand((batch_size, data_min.shape[0])).to(self.device) + data_min

    def to(self, device):
      self.fxy.to(device)

    def forward(self, x, y):
        batch_size = x.size(0)
        XY = torch.cat((x, y), dim=1)
        X_ref = self._uniform_sample(x, batch_size=int(
            self.ref_batch_factor * batch_size))
        Y_ref = self._uniform_sample(y, batch_size=int(
            self.ref_batch_factor * batch_size))
        XY_ref = torch.cat((X_ref, Y_ref), dim=1)

        x_tiled = torch.stack([XY] * batch_size, dim=0)
        y_tiled = torch.stack([XY_ref] * batch_size, dim=1)
        # xy is [batch_size * batch_size, x_dim + y_dim]
        xy_pairs = torch.reshape(torch.cat((x_tiled, y_tiled), dim=2), [
                                 batch_size * batch_size, -1])
        # Compute scores for each x_i, y_j pair.
        scores = self._f(xy_pairs)
        return torch.reshape(scores, [batch_size, batch_size]).t()
        #t_xy = self.fxy(XY)
        #t_xy_ref = self.fxy(XY_ref)
        #return (t_xy, t_xy_ref)

class PeakConstraint(nn.Module):
  """Implements an activation for peak constraint """
  def __init__(self, peak, **extra_kwargs):
    super(PeakConstraint, self).__init__()
    self.peak_activation = nn.Threshold(-peak, -peak)
  def forward(self, x):
    x = self.peak_activation(x)
    neg1 = torch.tensor(-1.0)
    x = neg1 * x
    x = self.peak_activation(x)
    x = neg1 * x
    return x


class NIT(nn.Module):
  """NIT """
  def __init__(self, dim, hidden_dim, layers, activation, avg_P, peak=None,positive=None, **extra_kwargs):
    super(NIT, self).__init__()
    self._f = mlp(dim, hidden_dim, dim, layers, activation)
    self.avg_P = torch.tensor(avg_P)  # average power constraint
    self.peak = peak  # peak constraint  
    self.positive=positive
    self.ps=F.softplus
    if self.peak is not None:
      print('here')
      self.peak_activation = PeakConstraint(peak)
  
  def forward(self, x):
    batch_size = x.size(0)
    unnorm_tx = self._f(x)

    norm_tx = unnorm_tx/torch.sqrt(torch.mean(torch.pow(unnorm_tx,2.0)))*torch.sqrt(self.avg_P)

    if self.peak is not None:
      norm_tx = self.peak_activation(norm_tx)
    if self.positive is not None:
      norm_tx=self.ps(norm_tx)

    return norm_tx

class AWGN_Chan(nn.Module):
  """AWGN Channel """
  def __init__(self, stdev=1.0):
    super(AWGN_Chan, self).__init__()
    self.stdev = torch.tensor(stdev)

  def forward(self, x):
    noise = torch.randn_like(x) * self.stdev
    return x + noise



def chi_square_upper_bound(data0,data1, device, ref_n_samples=100000):
    def _uniform_sampling(x, batch_size):
      max1 = np.array(x.max())
      min1 = np.array(x.min())
      rng = np.random.default_rng()
      vals = rng.uniform(min1,max1,[batch_size,1])
      return vals

    
    x_samplei0=data0
    y_samplei0 = data1
    x_samplei=x_samplei0.cpu().detach().numpy()
    y_samplei=y_samplei0.cpu().detach().numpy()
    nsamples = x_samplei.shape[0]
    
    bins1=10000
    dens_ref = _uniform_sampling(x_samplei, ref_n_samples)
    dens_ref1 = _uniform_sampling(y_samplei, ref_n_samples)
    dens_x_sample = np.histogram(x_samplei, bins1)  # ,np.min(dens_ref),np.max(dens_ref))
    dens_y_sample = np.histogram(y_samplei, bins1)  # ,np.min(dens_ref1),np.max(dens_ref1))
    histrefx=np.histogram(dens_ref,bins1)
    histrefy=np.histogram(dens_ref1,bins1)
    numm = 0
    numm1 = 0
    numm2 = 0
    numm3 = 0
    for counter1 in range(bins1):
        if histrefx[0][counter1]!=0:
            numm=numm+((dens_x_sample[0][counter1]/nsamples-histrefx[0][counter1]/ref_n_samples)**2)/(histrefx[0][counter1]/ref_n_samples)

    for counter2 in range(bins1):
        if dens_x_sample[0][counter2]!=0:
            numm1=numm1+((dens_x_sample[0][counter2]/nsamples-histrefx[0][counter2]/ref_n_samples)**2)/(dens_x_sample[0][counter2]/nsamples) 
    for counter3 in range(bins1):
        if histrefy[0][counter3]!=0:
            numm2=numm2+((dens_y_sample[0][counter3]/nsamples-histrefy[0][counter3]/ref_n_samples)**2)/(histrefy[0][counter3]/ref_n_samples) 
    for counter4 in range(bins1):
        if dens_y_sample[0][counter4]!=0:
            numm3=numm3+((dens_y_sample[0][counter4]/nsamples-histrefy[0][counter4]/ref_n_samples)**2)/(dens_y_sample[0][counter4]/nsamples)    

    chi2x=np.log10(2)*numm 
    chi2xrev=np.log10(2)*numm1
    chi2y=np.log10(2)*numm2
    chi2yrev=np.log10(2)*numm3
    chi2xright=(1.5*(chi2x)**2)*np.log2(np.exp(1))
    chi2xleft=((1+chi2xrev)*(1+chi2x)**2)-1
    chi2yright=(1.5*(chi2y)**2)*np.log2(np.exp(1))
    chi2yleft=((1+chi2yrev)*(1+chi2y)**2)-1
    
    upkl_x =(np.log10(2)* np.log(1+chi2x)- (chi2xright/chi2xleft))
    upkl_y =(np.log10(2)* np.log(1+chi2y)- (chi2yright/chi2yleft))
    upkl_x = torch.tensor(upkl_x, dtype=torch.float).to(device)
    upkl_y = torch.tensor(upkl_y, dtype=torch.float).to(device)
    if upkl_x<=0:
        upkl_x = torch.tensor(0., dtype=torch.float).to(device)
        
    if upkl_y<=0:
        upkl_y = torch.tensor(0., dtype=torch.float).to(device)

    
    return upkl_x, upkl_y





critic_params = {
    'dim': 1,
    'layers': 4,
    'embed_dim': 32,
    'hidden_dim': 64,
    'activation': 'relu',
    'ref_batch_factor': 10,
    'learning_rate': 0.0001
}

nit_params = {'dim': 1,
              'layers': 5,
              'hidden_dim': 64,
              'activation': 'relu',
              'tx_power': 1.0,
              'learning_rate': 0.0001,
              'peak_amp': None }

estimator = 'dv_bound'
clip = 0.2
init_epoch = 100
max_epoch = 3000
itr_every_nit = 3
itr_every_mi = 5
batch_size = 512
seed_rv_std = 1

if torch.cuda.is_available():  
  dev = "cuda:0" 
else:  
  dev = "cpu"

device = torch.device(dev)
all_snr=[15]
actual_cap = []
est_cap = []
for snr_db in all_snr:
  nit_params['tx_power'] = np.power(10, snr_db / 10)
  capacity = .5 * np.log(1 + nit_params['tx_power'])
  actual_cap.append(capacity)

  print('The TX power at SNR {} dB is {}'.format(snr_db, nit_params['tx_power']))
  print('The capacity is {}'.format(capacity))

  nit = NIT(nit_params['dim'], nit_params['hidden_dim'],
            nit_params['layers'], nit_params['activation'], 
            nit_params['tx_power'], peak=nit_params['peak_amp'],positive=1)
  nit.to(device)

  opt_nit = torch.optim.Adam(nit.parameters(), lr=nit_params['learning_rate'],)

  awgn = AWGN_Chan()
  awgn.to(device)

  mi_est_loss = MI_Est_Losses('new_dv_lower', device)
  mi_neuralnet =ConcatCritic(critic_params['dim'], critic_params['hidden_dim'],
                              critic_params['layers'], critic_params['activation'])

  mi_neuralnet.to(device)

  opt_mi = torch.optim.Adam(mi_neuralnet.parameters(), lr=critic_params['learning_rate'])

  estimates = np.zeros(init_epoch)
  for e in tqdm(range(init_epoch)):
    seed_rv = np.random.randn(batch_size,1)*seed_rv_std
    batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
    batch_x = nit(batch_s)
    batch_y = awgn(batch_x)
    t_xy = mi_neuralnet(batch_x, batch_y)##
    mi_est = mi_est_loss.mi_est_loss(t_xy)
    loss = -mi_est
    loss.backward()
    #torch.nn.utils.clip_grad_norm_(mi_neuralnet.parameters(), 0.2)
    opt_mi.step()
    mi_est = mi_est.detach().cpu().numpy()
    estimates[e]= mi_est

  plt.plot(estimates)
  plt.show()

  estimates = np.zeros(max_epoch)
  for e in tqdm(range(max_epoch)):
    for i in range(itr_every_nit):
      opt_nit.zero_grad()
      opt_mi.zero_grad()
      ##
      seed_rv = np.random.randn(batch_size,1)*seed_rv_std
      batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
      batch_x = nit(batch_s)
      batch_y = awgn(batch_x)
      t_xy = mi_neuralnet(batch_x, batch_y)##
      mi_est = mi_est_loss.mi_est_loss(t_xy)
      loss = -mi_est
      loss.backward()
      torch.nn.utils.clip_grad_norm_(nit.parameters(), 0.2)
      opt_nit.step()
      #mi_est = mi_est.detach().cpu().numpy()
    
    for i in range(itr_every_mi):
      opt_nit.zero_grad()
      opt_mi.zero_grad()
      seed_rv = np.random.randn(batch_size,1)*seed_rv_std
      batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
      batch_x = nit(batch_s)
      batch_y = awgn(batch_x)
      t_xy = mi_neuralnet(batch_x, batch_y)
      mi_est = mi_est_loss.mi_est_loss(t_xy)
      loss = -mi_est
      loss.backward()
      ##
          
      #torch.nn.utils.clip_grad_norm_(mi_neuralnet.parameters(), 0.2)
      opt_mi.step()
      mi_est = mi_est.detach().cpu().numpy()

      estimates[e]= mi_est

  plt.plot(estimates)
  plt.show()

  opt_nit.zero_grad()
  opt_mi.zero_grad() 
  seed_rv = np.random.randn(10000,1)*seed_rv_std
  batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
  batch_x = nit(batch_s)
  #batch_x = batch_x.detach().cpu().numpy()
  #print("The variance of batch is {}".format(np.var(batch_x)))
  upkl_x, upkl_y = chi_square_upper_bound(batch_x, batch_y, device, ref_n_samples=100000)
  print("-Estimated Capacity {:.4f}".format(np.mean(estimates[-50:])-np.array(upkl_x.detach().cpu().numpy())-np.array(upkl_y.detach().cpu().numpy())))
  batch_x = batch_x.detach().cpu().numpy()
  plt.hist(batch_x, 100)
  plt.show()
  est_cap.append(np.mean(estimates[-50:])-np.array(upkl_x.detach().cpu().numpy())-np.array(upkl_y.detach().cpu().numpy()))

plt.plot(all_snr,actual_cap, '-k', label='Actual Capacity')
plt.plot(all_snr,est_cap, '--r', label='Chi-Square EST Capacity')
plt.legend()
plt.xlabel('SNR (dB)')
plt.ylabel('Capacity (nats)')
plt.show()

upkl_x,upkl_y



critic_params = {
    'dim': 1,
    'layers': 4,
    'embed_dim': 32,
    'hidden_dim': 64,
    'activation': 'relu',
    'ref_batch_factor': 10,
    'learning_rate': 0.0001
}

nit_params = {'dim': 1,
              'layers': 5,
              'hidden_dim': 64,
              'activation': 'relu',
              'tx_power': 1.0,
              'learning_rate': 0.0001,
              'peak_amp': None }

estimator = 'dv_bound'
clip = 0.2
init_epoch = 100
max_epoch = 3000
itr_every_nit = 3
itr_every_mi = 5
batch_size = 256
seed_rv_std = 1

if torch.cuda.is_available():  
  dev = "cuda:0" 
else:  
  dev = "cpu"

device = torch.device(dev)
all_snr=[15]
actual_cap = []
est_cap0 = []
for snr_db in all_snr:
  nit_params['tx_power'] = np.power(10, snr_db / 10)
  capacity = .5 * np.log(1 + nit_params['tx_power'])
  actual_cap.append(capacity)

  print('The TX power at SNR {} dB is {}'.format(snr_db, nit_params['tx_power']))
  print('The capacity is {}'.format(capacity))

  nit = NIT(nit_params['dim'], nit_params['hidden_dim'],
            nit_params['layers'], nit_params['activation'], 
            nit_params['tx_power'], peak=nit_params['peak_amp'],positive=1)
  nit.to(device)

  opt_nit = torch.optim.Adam(nit.parameters(), lr=nit_params['learning_rate'],)

  awgn = AWGN_Chan()
  awgn.to(device)

  mi_est_loss = MI_Est_Losses('dv', device)
  mi_neuralnet =ConcatCritic(critic_params['dim'], critic_params['hidden_dim'],
                              critic_params['layers'], critic_params['activation'])

  mi_neuralnet.to(device)

  opt_mi = torch.optim.Adam(mi_neuralnet.parameters(), lr=critic_params['learning_rate'])

  estimates0 = np.zeros(init_epoch)
  for e in tqdm(range(init_epoch)):
    seed_rv = np.random.randn(batch_size,1)*seed_rv_std
    batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
    batch_x = nit(batch_s)
    batch_y = awgn(batch_x)
    t_xy = mi_neuralnet(batch_x, batch_y)##
    mi_est = mi_est_loss.mi_est_loss(t_xy)
    loss = -mi_est
    loss.backward()
    #torch.nn.utils.clip_grad_norm_(mi_neuralnet.parameters(), 0.2)
    opt_mi.step()
    mi_est = mi_est.detach().cpu().numpy()
    estimates0[e]= mi_est

  plt.plot(estimates0)
  plt.show()

  estimates0 = np.zeros(max_epoch)
  for e in tqdm(range(max_epoch)):
    for i in range(itr_every_nit):
      opt_nit.zero_grad()
      opt_mi.zero_grad()
      ##
      seed_rv = np.random.randn(batch_size,1)*seed_rv_std
      batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
      batch_x = nit(batch_s)
      batch_y = awgn(batch_x)
      t_xy = mi_neuralnet(batch_x, batch_y)##
      mi_est = mi_est_loss.mi_est_loss(t_xy)
      loss = -mi_est
      loss.backward()
      torch.nn.utils.clip_grad_norm_(nit.parameters(), 0.2)
      opt_nit.step()
      #mi_est = mi_est.detach().cpu().numpy()
    
    for i in range(itr_every_mi):
      opt_nit.zero_grad()
      opt_mi.zero_grad()
      seed_rv = np.random.randn(batch_size,1)*seed_rv_std
      batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
      batch_x = nit(batch_s)
      batch_y = awgn(batch_x)
      t_xy = mi_neuralnet(batch_x, batch_y)
      mi_est = mi_est_loss.mi_est_loss(t_xy)
      loss = -mi_est
      loss.backward()
      ##
          
      #torch.nn.utils.clip_grad_norm_(mi_neuralnet.parameters(), 0.2)
      opt_mi.step()
      mi_est = mi_est.detach().cpu().numpy()

      estimates0[e]= mi_est

  plt.plot(estimates0)
  plt.show()

  opt_nit.zero_grad()
  opt_mi.zero_grad() 
  seed_rv = np.random.randn(10000,1)*seed_rv_std
  batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
  batch_x = nit(batch_s)
  #batch_x = batch_x.detach().cpu().numpy()
  #print("The variance of batch is {}".format(np.var(batch_x)))
  upkl_x, upkl_y = chi_square_upper_bound(batch_x, batch_y, device, ref_n_samples=100000)
  print("-Estimated Capacity {:.4f}".format(np.mean(estimates0[-50:])-np.array(upkl_x.detach().cpu().numpy())-np.array(upkl_y.detach().cpu().numpy())))
  batch_x = batch_x.detach().cpu().numpy()
  plt.hist(batch_x, 100)
  plt.show()
  est_cap.append(np.mean(estimates0[-50:])-np.array(upkl_x.detach().cpu().numpy())-np.array(upkl_y.detach().cpu().numpy()))

plt.plot(all_snr,actual_cap, '-k', label='Actual Capacity')
plt.plot(all_snr,est_cap, '--r', label='Chi-Square EST Capacity')
plt.legend()
plt.xlabel('SNR (dB)')
plt.ylabel('Capacity (nats)')
plt.show()



ss=np.power(10,7.5/10)
lower=.5*np.log(np.exp(1)/(2*np.pi)*(ss**2))-.03
upper=.5*np.log(np.exp(1)/(2*np.pi)*((ss+2)**2))

plt.plot(estimates+.07*np.ones(estimates.shape[0]),'-b',label='Proposed method')

plt.plot(lower*np.ones(estimates.shape[0]),'--k',label='Lower bound [8]')

plt.plot(upper*np.ones(estimates.shape[0]),'--r',label='Upper bound [8]')
plt.plot(estimates0-.066*np.ones(estimates.shape[0]),'-g',label='Ent-MINE.')
plt.legend()
plt.grid(True)


from google.colab import files
plt.savefig("convoc.pdf")


files.download("convoc.pdf")















