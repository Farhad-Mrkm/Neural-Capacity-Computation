# -*- coding: utf-8 -*-
"""Joint_Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ieYiMsnPHSE1NEN2_FnyCOudJ1KVkykw
"""

def joint_train(critic_params,nit_params,estimator,all_snr,batch_size,init_epoch,positive=None):
  init_epoch = init_epoch
  max_epoch = 9000
  itr_every_nit = 2
  itr_every_mi = 5
  batch_size = batch_size
  seed_rv_std = 1

  if torch.cuda.is_available():  
    dev = "cuda:0" 
  else:  
    dev = "cpu"

  device = torch.device(dev)
  all_snr = all_snr
  actual_cap = []
  est_cap = []
  for snr_db in all_snr:
    nit_params['tx_power'] = np.power(10, snr_db / 10)
    capacity = .5 * np.log(1 + nit_params['tx_power'])
    actual_cap.append(capacity)

    print('The TX power at SNR {} dB is {}'.format(snr_db, nit_params['tx_power']))
    print('The capacity is {}'.format(capacity))

    nit = NIT(nit_params['dim'], nit_params['hidden_dim'],
              nit_params['layers'], nit_params['activation'], 
              nit_params['tx_power'], peak=nit_params['peak_amp'],positive=positive)#None for AWGN_other for  oimac.
    nit.to(device)

    opt_nit = torch.optim.Adam(nit.parameters(), lr=nit_params['learning_rate'])

    awgn = AWGN_Chan()
    awgn.to(device)

    mi_est_loss = MI_Est_Losses('estimator', device)
    mi_neuralnet = ConcatCritic(critic_params['dim'], critic_params['hidden_dim'],
                                critic_params['layers'], critic_params['activation'])

    mi_neuralnet.to(device)

    opt_mi = torch.optim.Adam(mi_neuralnet.parameters(), lr=critic_params['learning_rate'])

    estimates = np.zeros(init_epoch)
  for e in tqdm(range(init_epoch)):
    seed_rv = np.random.randn(batch_size,1)*seed_rv_std
    batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
    batch_x = nit(batch_s)
    batch_y = awgn(batch_x)
    t_xy = mi_neuralnet(batch_x, batch_y)##
    mi_est = mi_est_loss.mi_est_loss(t_xy)
    loss = -mi_est
    loss.backward()
    #torch.nn.utils.clip_grad_norm_(mi_neuralnet.parameters(), 0.2)
    opt_mi.step()
    mi_est = mi_est.detach().cpu().numpy()
    estimates[e]= mi_est

  plt.plot(estimates)
  plt.show()

  estimates = np.zeros(max_epoch)
  for e in tqdm(range(max_epoch)):
    for i in range(itr_every_nit):
      opt_nit.zero_grad()
      opt_mi.zero_grad()
      ##
      seed_rv = np.random.randn(batch_size,1)*seed_rv_std
      batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
      batch_x = nit(batch_s)
      batch_y = awgn(batch_x)
      t_xy = mi_neuralnet(batch_x, batch_y)##
      mi_est = mi_est_loss.mi_est_loss(t_xy)
      loss = -mi_est
      loss.backward()
      #torch.nn.utils.clip_grad_norm_(nit.parameters(), 0.2)
      opt_nit.step()
      #mi_est = mi_est.detach().cpu().numpy()
    
    for i in range(itr_every_mi):
      opt_nit.zero_grad()
      opt_mi.zero_grad()
      seed_rv = np.random.randn(batch_size,1)*seed_rv_std
      batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
      batch_x = nit(batch_s)
      batch_y = awgn(batch_x)
      t_xy = mi_neuralnet(batch_x, batch_y)
      mi_est = mi_est_loss.mi_est_loss(t_xy)
      loss = -mi_est
      loss.backward()
      ##
          
      #torch.nn.utils.clip_grad_norm_(mi_neuralnet.parameters(), 0.2)
      opt_mi.step()
      mi_est = mi_est.detach().cpu().numpy()

      estimates[e]= mi_est

  plt.plot(estimates)
  plt.show()

  opt_nit.zero_grad()
  opt_mi.zero_grad() 
  seed_rv = np.random.randn(10000,1)*seed_rv_std
  batch_s = torch.tensor(seed_rv, dtype=torch.float).to(device) 
  batch_x = nit(batch_s)
  batch_x = batch_x.detach().cpu().numpy()
  print("The variance of batch is {}".format(np.var(batch_x)))
  print("-Estimated Capacity {:.4f}".format(np.mean(estimates[-50:])))

  plt.show()
  est_cap.append(np.mean(estimates[-50:]))

  plt.plot(all_snr,actual_cap, '-k', label='Actual Capacity')
  plt.plot(all_snr,est_cap, '--r', label='SMILE EST Capacity')
  plt.legend()
  plt.xlabel('SNR (dB)')
  plt.ylabel('Capacity (nats)')
  plt.show()