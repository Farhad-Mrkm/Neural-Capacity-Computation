# -*- coding: utf-8 -*-
"""(CHI_SQUARE_Comparison00)__.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mYzDUb3igdLaRbh4eczRelQTIYCzXKCV
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib notebook
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
from tqdm.auto import tqdm, trange
from numpy.random import default_rng
import torch.nn.functional as F

def logmeanexp_diag(x, device):
    """Compute logmeanexp over the diagonal elements of x."""
    batch_size = x.size(0)

    logsumexp = torch.logsumexp(x.diag(), dim=(0,))
    num_elem = batch_size

    return logsumexp - torch.log(torch.tensor(num_elem).float()).to(device)


def logmeanexp_nodiag(x, device, dim=None):
    batch_size = x.size(0)
    if dim is None:
        dim = (0, 1)

    logsumexp = torch.logsumexp(
        x - torch.diag(np.inf * torch.ones(batch_size).to(device)), dim=dim)

    try:
        if len(dim) == 1:
            num_elem = batch_size - 1.
        else:
            num_elem = batch_size * (batch_size - 1.)
    except ValueError:
        num_elem = batch_size - 1
    return logsumexp - torch.log(torch.tensor(num_elem)).to(device)


def tuba_lower_bound(scores, device, log_baseline=None):
    if log_baseline is not None:
        scores -= log_baseline[:, None]

    # First term is an expectation over samples from the joint,
    # which are the diagonal elmements of the scores matrix.
    joint_term = scores.diag().mean()

    # Second term is an expectation over samples from the marginal,
    # which are the off-diagonal elements of the scores matrix.
    marg_term = logmeanexp_nodiag(scores, device).exp()
    return 1. + joint_term - marg_term


def nwj_lower_bound(scores, device):
    return tuba_lower_bound(scores - 1., device)


def infonce_lower_bound(scores):
    nll = scores.diag().mean() - scores.logsumexp(dim=1)
    # Alternative implementation:
    # nll = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=tf.range(batch_size))
    mi = torch.tensor(scores.size(0)).float().log() + nll
    mi = mi.mean()
    return mi


def js_fgan_lower_bound(f):
    """Lower bound on Jensen-Shannon divergence from Nowozin et al. (2016)."""
    f_diag = f.diag()
    first_term = -F.softplus(-f_diag).mean()
    n = f.size(0)
    second_term = (torch.sum(F.softplus(f)) -
                   torch.sum(F.softplus(f_diag))) / (n * (n - 1.))
    return first_term - second_term


def js_lower_bound(f, device):
    """Obtain density ratio from JS lower bound then output MI estimate from NWJ bound."""
    nwj = nwj_lower_bound(f, device)
    js = js_fgan_lower_bound(f)

    with torch.no_grad():
        nwj_js = nwj - js

    return js + nwj_js


def dv_upper_lower_bound(f, device):
    """
    Donsker-Varadhan lower bound, but upper bounded by using log outside.
    Similar to MINE, but did not involve the term for moving averages.
    """
    first_term = f.diag().mean()
    second_term = logmeanexp_nodiag(f, device)

    return first_term - second_term


def mine_lower_bound(f, device, buffer=None, momentum=0.9):
    """
    MINE lower bound based on DV inequality.
    """
    if buffer is None:
        buffer = torch.tensor(1.0).to(device)
    first_term = f.diag().mean()

    buffer_update = logmeanexp_nodiag(f, device).exp()
    with torch.no_grad():
        second_term = logmeanexp_nodiag(f, device)
        buffer_new = buffer * momentum + buffer_update * (1 - momentum)
        buffer_new = torch.clamp(buffer_new, min=1e-4)
        third_term_no_grad = buffer_update / buffer_new

    third_term_grad = buffer_update / buffer_new

    return first_term - second_term - third_term_grad + third_term_no_grad, buffer_update


def smile_lower_bound(f, device, clip=None):
    if clip is not None:
        f_ = torch.clamp(f, -clip, clip)
    else:
        f_ = f
    z = logmeanexp_nodiag(f_, device, dim=(0, 1))
    dv = f.diag().mean() - z

    js = js_fgan_lower_bound(f)

    with torch.no_grad():
        dv_js = dv - js

    return js + dv_js


def _ent_js_fgan_lower_bound(vec, ref_vec):
    """Lower bound on Jensen-Shannon divergence from Nowozin et al. (2016)."""
    first_term = -F.softplus(-vec).mean()
    second_term = torch.sum(F.softplus(ref_vec)) / ref_vec.size(0)
    return first_term - second_term

def _ent_smile_lower_bound(vec, ref_vec, device, clip=None):
    if clip is not None:
        ref = torch.clamp(ref_vec, -clip, clip)
    else:
        ref = ref_vec

    batch_size = ref.size(0)
    z = log_mean_ef_ref = torch.logsumexp(ref, dim=(0, 1)) - torch.log(torch.tensor(batch_size)).to(device)
    dv = vec.mean() - z
    js = _ent_js_fgan_lower_bound(vec, ref_vec)

    with torch.no_grad():
        dv_js = dv - js
    
    return js + dv_js
def chisquare_pred(vec, ref_vec, device,beta=.9):
    beta = beta
    z = log_mean_ef_ref = (torch.mean(torch.exp(ref_vec))/(torch.tensor(beta)))-torch.log(torch.tensor(beta))-1
    dv = vec.mean() - z
    js = _ent_js_fgan_lower_bound(vec, ref_vec)

    with torch.no_grad():
        dv_js = dv - js
    
    return js + dv_js

def entropic_smile_lower_bound(f, device, clip=None):
    t_xy, t_xy_ref, t_x, t_x_ref, t_y, t_y_ref = f

    d_xy = _ent_smile_lower_bound(t_xy, t_xy_ref, device, clip=clip)
    d_x = _ent_smile_lower_bound(t_x, t_x_ref, device, clip=clip)
    d_y = _ent_smile_lower_bound(t_y, t_y_ref, device, clip=clip)

    return d_xy, d_x, d_y
def chi_square_lower_bound(f,device,beta=.9): 
     x, y = f

     d_xy = chisquare_pred(x, y, device,beta=.9)
   
     return d_xy

def estimate_mutual_information(estimator, x, y, critic_fn, device, critic_type='ent_concat',
                                baseline_fn=None, alpha_logit=None, **kwargs):
    """Estimate variational lower bounds on mutual information.

  Args:
    estimator: string specifying estimator, one of:
      'nwj', 'infonce', 'tuba', 'js', 'interpolated'
    x: [batch_size, dim_x] Tensor
    y: [batch_size, dim_y] Tensor
    critic_fn: callable that takes x and y as input and outputs critic scores
      output shape is a [batch_size, batch_size] matrix
    baseline_fn (optional): callable that takes y as input
      outputs a [batch_size]  or [batch_size, 1] vector
    alpha_logit (optional): logit(alpha) for interpolated bound

  Returns:
    scalar estimate of mutual information
    """
    x = torch.tensor(x, dtype=torch.float).to(device) 
    y = torch.tensor(y, dtype=torch.float).to(device)
    #x, y = x.to(device), y.to(device)
    if critic_type=='ent_concat':
      scores = critic_fn.forward(x,y)

    elif critic_type=='chi_square':
        scores=critic_fn.forward(x,y)
    else:
      scores = critic_fn(x, y)

    if baseline_fn is not None:
        # Some baselines' output is (batch_size, 1) which we remove here.
        log_baseline = torch.squeeze(baseline_fn(y))
    if estimator == 'infonce':
        mi = infonce_lower_bound(scores)
    elif estimator == 'nwj':
        mi = nwj_lower_bound(scores, device)
    elif estimator == 'tuba':
        mi = tuba_lower_bound(scores, device, log_baseline)
    elif estimator == 'js':
        mi = js_lower_bound(scores, device)
    elif estimator == 'smile':
        mi = smile_lower_bound(scores, device, **kwargs)
    elif estimator == 'dv':
        mi = dv_upper_lower_bound(scores, device)
    elif estimator == 'ent_smile':
        mi = entropic_smile_lower_bound(scores, device, **kwargs)
    elif estimator =='chi_square':
        mi = chi_square_lower_bound(scores,device,**kwargs)
    return mi

def _uniform_sampling(x, batch_size):
    max1 = np.array(x.max())
    min1 = np.array(x.min())
    rng = np.random.default_rng()
    vals = rng.uniform(min1,max1,[batch_size,1])
    return vals

def gen_gaussian(numb_samples, var):
    return np.random.normal(0., np.sqrt(var), [numb_samples, 1])

def awgn_chan(numb_samples, tx_power, noise_power, x_inp=None):
    if x_inp is None:
        x = gen_gaussian(numb_samples, tx_power)
    else:
        x = x_inp

    y = x + gen_gaussian(numb_samples, noise_power)
    return x, y

def mlp(dim, hidden_dim, output_dim, layers, activation):
    """Create a mlp from the configurations."""
    activation = {
        'relu': nn.ReLU
    }[activation]

    seq = [nn.Linear(dim, hidden_dim), activation()]
    for _ in range(layers):
        seq += [nn.Linear(hidden_dim, hidden_dim), activation()]
    seq += [nn.Linear(hidden_dim, output_dim)]

    return nn.Sequential(*seq)


class SeparableCritic(nn.Module):
    """Separable critic. where the output value is g(x) h(y). """

    def __init__(self, dim, hidden_dim, embed_dim, layers, activation, **extra_kwargs):
        super(SeparableCritic, self).__init__()
        self._g = mlp(dim, hidden_dim, embed_dim, layers, activation)
        self._h = mlp(dim, hidden_dim, embed_dim, layers, activation)

    def forward(self, x, y):
        scores = torch.matmul(self._h(y), self._g(x).t())
        return scores


class ConcatCritic(nn.Module):
    """Concat critic, where we concat the inputs and use one MLP to output the value."""

    def __init__(self, dim, hidden_dim, layers, activation, **extra_kwargs):
        super(ConcatCritic, self).__init__()
        # output is scalar score
        self._f = mlp(dim * 2, hidden_dim, 1, layers, activation)

    def forward(self, x, y):
        batch_size = x.size(0)
        # Tile all possible combinations of x and y
        x_tiled = torch.stack([x] * batch_size, dim=0)
        y_tiled = torch.stack([y] * batch_size, dim=1)
        # xy is [batch_size * batch_size, x_dim + y_dim]
        xy_pairs = torch.reshape(torch.cat((x_tiled, y_tiled), dim=2), [
                                 batch_size * batch_size, -1])
        # Compute scores for each x_i, y_j pair.
        scores = self._f(xy_pairs)
        return torch.reshape(scores, [batch_size, batch_size]).t()

class EntropicCritic(nn.Module):
  # Inner class that defines the neural network architecture
  def __init__(self, dim, hidden_dim, embed_dim, layers, activation):
      super(EntropicCritic, self).__init__()
      self.f = mlp(dim, hidden_dim, embed_dim, layers, activation)

  def forward(self, inp):
      output = self.f(inp)
      return output


class ConcatEntropicCritic():
    """Concat entropic critic, where we concat the inputs and use one MLP to output the value."""
    def __init__(self, dim, hidden_dim, layers, activation, 
                 device, ref_batch_factor=1,**extra_kwargs):
        # output is scalar score
        self.ref_batch_factor = ref_batch_factor
        self.fxy = EntropicCritic(dim * 2, hidden_dim, 1, layers, activation)
        self.fx = EntropicCritic(dim, hidden_dim, 1, layers, activation)
        self.fy = EntropicCritic(dim, hidden_dim, 1, layers, activation)
        self.device = device

    def _uniform_sample(self, data, batch_size):
      # Sample the reference uniform distribution
      data_min = data.min(dim=0)[0]
      data_max = data.max(dim=0)[0]
      return (data_max - data_min) * torch.rand((batch_size, data_min.shape[0])).to(self.device) + data_min

    def to(self, device):
      self.fxy.to(device)
      self.fx.to(device)
      self.fy.to(device)

    def forward(self, x, y):
        batch_size = x.size(0)
        XY = torch.cat((x, y), dim=1)
        X_ref = self._uniform_sample(x, batch_size=int(
            self.ref_batch_factor * batch_size))
        Y_ref = self._uniform_sample(y, batch_size=int(
            self.ref_batch_factor * batch_size))
        XY_ref = torch.cat((X_ref, Y_ref), dim=1)

        # Compute t function outputs approximated by NNs
        t_xy = self.fxy(XY)
        t_xy_ref = self.fxy(XY_ref)
        t_x = self.fx(x)
        t_x_ref = self.fx(X_ref)
        t_y = self.fy(y)
        t_y_ref = self.fy(Y_ref)
        return (t_xy, t_xy_ref, t_x, t_x_ref, t_y, t_y_ref)




### ne critic

class chiCritic():
    """Concat entropic critic, where we concat the inputs and use one MLP to output the value."""
    def __init__(self, dim, hidden_dim, layers, activation, 
                 device, ref_batch_factor=1,**extra_kwargs):
        # output is scalar score
        self.ref_batch_factor = ref_batch_factor
        self.fxy = EntropicCritic(dim * 2, hidden_dim, 1, layers, activation)
        
        
        self.device = device

    def _uniform_sample(self, data, batch_size):
      # Sample the reference uniform distribution
      data_min = data.min(dim=0)[0]
      data_max = data.max(dim=0)[0]
      return (data_max - data_min) * torch.rand((batch_size, data_min.shape[0])).to(self.device) + data_min

    def to(self, device):
      self.fxy.to(device)
      
      

    def forward(self, x, y):
        batch_size = x.size(0)
        XY = torch.cat((x, y), dim=1)
        X_ref = self._uniform_sample(x, batch_size=int(
            self.ref_batch_factor * batch_size))
        Y_ref = self._uniform_sample(y, batch_size=int(
            self.ref_batch_factor * batch_size))
        XY_ref = torch.cat((X_ref, Y_ref), dim=1)

        # Compute t function outputs approximated by NNs
        t_xy = self.fxy(XY)
        t_xy_ref = self.fxy(XY_ref)
        return (t_xy, t_xy_ref)

def train_estimator(critic_params, data_params, opt_params, device, 
                    critic_type, **kwargs):
    """Main training loop that estimates time-varying MI."""
    # Ground truth rho is only used by conditional critic
    if critic_type=='concat':
        critic = ConcatCritic(critic_params['dim'], critic_params['hidden_dim'],
                              critic_params['layers'], critic_params['activation'])
    elif critic_type=='ent_concat':
        critic = ConcatEntropicCritic(critic_params['dim'], critic_params['hidden_dim'],
                              critic_params['layers'], critic_params['activation'],
                              device, critic_params['ref_batch_factor'])
    elif critic_type=='chi_square':
        critic=chiCritic(critic_params['dim'], critic_params['hidden_dim'],
                              critic_params['layers'], critic_params['activation'],
                              device, critic_params['ref_batch_factor'])
    else:
        critic = SeparableCritic(critic_params['dim'], critic_params['hidden_dim'], critic_params['embed_dim'],
                              critic_params['layers'], critic_params['activation'])

    critic.to(device)
    if critic_type=='ent_concat':
      opt_ent_xy = torch.optim.Adam(critic.fxy.parameters(), lr=opt_params['learning_rate'])
      opt_ent_x = torch.optim.Adam(critic.fx.parameters(), lr=opt_params['learning_rate'])
      opt_ent_y = torch.optim.Adam(critic.fy.parameters(), lr=opt_params['learning_rate'])

    elif critic_type=='chi_square':
        opt_crit = torch.optim.Adam(critic.fxy.parameters(), lr=opt_params['learning_rate'])
    else:
        opt_crit = torch.optim.Adam(critic.parameters(), lr=opt_params['learning_rate'])
    x_samplei, y_samplei = awgn_chan(100000, data_params['tx_power'], data_params['noise_power'])
    if data_params['tx_power']<=np.power(10,30/10):
        bins1=1000
    else:
        bins1=7000
    dens_ref = _uniform_sampling(x_samplei, 100000)
    dens_ref1 = _uniform_sampling(y_samplei, 100000)
    dens_x_sample = np.histogram(x_samplei, bins1)  # ,np.min(dens_ref),np.max(dens_ref))
    dens_y_sample = np.histogram(y_samplei, bins1)  # ,np.min(dens_ref1),np.max(dens_ref1))
    histrefx=np.histogram(dens_ref,bins1)
    histrefy=np.histogram(dens_ref1,bins1)
    numm=0
    numm1=0
    numm2=0
    numm3=0
    for counter1 in range(bins1):
        if histrefx[0][counter1]!=0:
            numm=numm+((dens_x_sample[0][counter1]/1000000-histrefx[0][counter1]/100000)**2)/(histrefx[0][counter1]/100000)

    for counter2 in range(bins1):
        if dens_x_sample[0][counter2]!=0:
            numm1=numm1+((dens_x_sample[0][counter2]/1000000-histrefx[0][counter2]/100000)**2)/(dens_x_sample[0][counter2]/1000000) 
    for counter3 in range(bins1):
        if histrefy[0][counter3]!=0:
            numm2=numm2+((dens_y_sample[0][counter3]/1000000-histrefy[0][counter3]/100000)**2)/(histrefy[0][counter3]/100000) 
    for counter4 in range(bins1):
        if dens_y_sample[0][counter1]!=0:
            numm3=numm3+((dens_y_sample[0][counter4]/1000000-histrefy[0][counter4]/100000)**2)/(dens_y_sample[0][counter4]/1000000)    

    chi2x=np.log10(2)*numm 
    chi2xrev=np.log10(2)*numm1
    chi2y=np.log10(2)*numm2
    chi2yrev=np.log10(2)*numm3
    chi2xright=(1.5*(chi2x)**2)*np.log2(np.exp(1))
    chi2xleft=((1+chi2xrev)*(1+chi2x)**2)-1
    chi2yright=(1.5*(chi2y)**2)*np.log2(np.exp(1))
    chi2yleft=((1+chi2yrev)*(1+chi2y)**2)-1
    if data_params['tx_power']<=np.power(10,50/10):
        upkl_x = np.log2(np.exp(1))*np.log2(1+chi2x)- np.log2(np.exp(1))*(chi2xright/chi2xleft)   
        upkl_y =np.log2(np.exp(1))*np.log2(1+chi2y)- np.log2(np.exp(1))*(chi2yright/chi2yleft)
        upkl_x = torch.tensor(upkl_x, dtype=torch.float)
        upkl_y = torch.tensor(upkl_y, dtype=torch.float)
    else:
        upkl_x = torch.tensor(0., dtype=torch.float)
        upkl_y = torch.tensor(0., dtype=torch.float)

    def train_step(data_params, opt_params, critic_type='ent_concat'):
        # Annoying special case:
        # For the true conditional, the critic depends on the true correlation rho,
        # so we rebuild the critic at each iteration.
        x, y = awgn_chan(data_params['batch_size'], data_params['tx_power'], data_params['noise_power'])
        if critic_type=='concat':
          opt_crit.zero_grad()
          mi = estimate_mutual_information(opt_params['estimator'], x, y, critic, device, 
                                           critic_type, **kwargs)
          loss = -mi
          loss.backward()
          torch.nn.utils.clip_grad_norm_(critic.parameters(), 2)
          opt_crit.step()
        elif critic_type=='chi_square':
            opt_crit.zero_grad()
            mi = estimate_mutual_information(opt_params['estimator'], x, y, critic, device, 
                                           critic_type, **kwargs)
            loss = -mi
            loss.backward()
            #torch.nn.utils.clip_grad_norm_(critic.fxy.parameters(), 2)
            opt_crit.step()
        else:
          opt_ent_xy.zero_grad()
          opt_ent_x.zero_grad()
          opt_ent_y.zero_grad()
          d_xy, d_x, d_y = estimate_mutual_information(opt_params['estimator'], x, y, critic, device, 
                                                      critic_type, **kwargs)
          loss_xy = -d_xy
          loss_x = -d_x
          loss_y = -d_y
          loss_xy.backward()
          torch.nn.utils.clip_grad_norm_(critic.fxy.parameters(), 2)
          opt_ent_xy.step()
          loss_x.backward()
          torch.nn.utils.clip_grad_norm_(critic.fx.parameters(), 2)
          opt_ent_x.step()
          loss_y.backward()
          torch.nn.utils.clip_grad_norm_(critic.fy.parameters(), 2)
          opt_ent_y.step()
          mi = d_xy - d_x - d_y
        if critic_type!='chi_square':
            return mi
        else:
            return mi-upkl_x-upkl_y



    estimates = []
    for i in tqdm(range(opt_params['iterations'])):
        mi = train_step(data_params, opt_params, critic_type=critic_type)
        mi = mi.detach().cpu().numpy()
        estimates.append(mi)

    return np.array(estimates)

if __name__ == "__main__":
    snr_db = 15

    if torch.cuda.is_available():  
      dev = "cuda:0" 
    else:  
      dev = "cpu"

    device = torch.device(dev)   

    critic_params = {
        'dim': 1,
        'layers': 4,
        'embed_dim': 32,
        'hidden_dim': 50,
        'activation': 'relu',
        'ref_batch_factor': 10
    }

    opt_params = {
        'iterations': 16000,
        'learning_rate': 2e-5,
        'estimator': 'ent_smile'
    }
    opt_param ={
        'iterations':6000,
        'learning_rate':2e-5,
        'estimator':'ent_smile',
    }

    
    all_snr_db=[1,5,10,20,30,40,50,60]


    ################################################
    ## Try SMILE estimation
    ################################################
    opt_params['estimator'] = 'smile'  
    all_gt_mi = []
    smile_est_mi = []
    for snr_db in all_snr_db:
        if snr_db<=40:
            opt_params['iterations']=6000
        elif snr_db<70:
            opt_params['iterations']=10000
        else:
            opt_params['iterations']=20000
            
        data_params = {
          'batch_size': 512,
          'noise_power': 1,
          'tx_power': np.power(10, snr_db / 10)
      }

        true_mi = .5 * np.log(1 + data_params['tx_power']/data_params['noise_power'])
        all_gt_mi.append(true_mi)
        est_mi = train_estimator(critic_params, data_params, opt_params, device, 'concat', clip=2.0)
        smile_est_mi.append([np.mean(est_mi[-30:]), np.std(est_mi[-30:])])
        print("-----Trained SNR {} for method {}: Actual MI {:.2f}, Estimated MI {:.2f}".format(snr_db, 
                                                                                'smile',
                                                                                true_mi,
                                                                                np.mean(est_mi[-30:])))
      
      ########################3
        ##chisquare
      #########################
    opt_params['estimator'] = 'chi_square'  
    all_gt_mi = []
    chisquare_est_mi = []
    for snr_db in all_snr_db:
        if snr_db>=50:
            data_params = {
            'batch_size': 15000,
            'noise_power': 1,
            'tx_power': np.power(10, snr_db / 10)
      }
        else:
            data_params = {
            'batch_size': 6000,
            'noise_power': 1,
            'tx_power': np.power(10, snr_db / 10)
      }
        if snr_db<=40:
            opt_params['iterations']=6000
        elif snr_db<70:
            opt_params['iterations']=10000
        else:
            opt_params['iterations']=20000
            

        true_mi = .5 * np.log(1 + data_params['tx_power']/data_params['noise_power'])
        all_gt_mi.append(true_mi)
        est_mi = train_estimator(critic_params, data_params, opt_params, device, 'chi_square')
        chisquare_est_mi.append([np.mean(est_mi[-100:]), np.std(est_mi[-100:])])
        print("-----Trained SNR {} for method {}: Actual MI {:.2f}, Estimated MI {:.2f}".format(snr_db, 
                                                                                'chi_square',
                                                                                true_mi,
                                                                                np.mean(est_mi[-100:])))

    ################################################
    ## Try MINE estimation
    ################################################ 
   # opt_params['estimator'] = 'smile'  
   # all_gt_mi = []
   # mine_est_mi = []
  #  for snr_db in all_snr_db:
  #    data_params = {
  #        'batch_size': 512,
  #        'noise_power': 1,
  #        'tx_power': np.power(10, snr_db / 10)
  #    }

 #     true_mi = .5 * np.log(1 + data_params['tx_power']/data_params['noise_power'])
 #     all_gt_mi.append(true_mi)
 #     est_mi = train_estimator(critic_params, data_params, opt_params, device, 'concat') ## no Clipping Gives MINE
 #     mine_est_mi.append([np.mean(est_mi[-30:]), np.std(est_mi[-30:])])
 #     print("-----Trained SNR {} for method {}: Actual MI {:.2f}, Estimated MI {:.2f}".format(snr_db, 
 #                                                                               'mine',
 #                                                                               true_mi,
 #                                                                               np.mean(est_mi[-30:])))


    ################################################
    ## Try Entropy-Based SMILE estimation
    ################################################
    opt_params['estimator'] = 'ent_smile'  
    ent_smile_est_mi = []
    for snr_db in all_snr_db:
        if snr_db<=40:
            opt_params['iterations']=6000
        elif snr_db<70:
            opt_params['iterations']=10000
        else:
            opt_params['iterations']=20000
            
        data_params = {
          'batch_size': 512,
          'noise_power': 1,
          'tx_power': np.power(10, snr_db / 10)
      }
        data_params = {
          'batch_size': 512,
          'noise_power': 1,
          'tx_power': np.power(10, snr_db / 10)
      }
        true_mi = .5 * np.log(1 + data_params['tx_power']/data_params['noise_power'])
        est_mi = train_estimator(critic_params, data_params, opt_params, device, 'ent_concat', clip=2.0)
        ent_smile_est_mi.append([np.mean(est_mi[-30:]), np.std(est_mi[-30:])])

        print("-----Trained SNR {} for method {}: Actual MI {:.2f}, Estimated MI {:.2f}".format(snr_db, 
                                                                                      'ent_smile',
                                                                                      true_mi,
                                                                                      np.mean(est_mi[-30:])))
      


    ################################################
    ## Try Entropy-Based MINE estimation
    ################################################
    #opt_params['estimator'] = 'ent_smile'  
    #ent_mine_est_mi = []
   # for snr_db in all_snr_db:
   #   data_params = {
   #       'batch_size': 512,
   #       'noise_power': 1,
   #       'tx_power': np.power(10, snr_db / 10)
  #    }
  #    true_mi = .5 * np.log(1 + data_params['tx_power']/data_params['noise_power'])
  #    est_mi = train_estimator(critic_params, data_params, opt_params, device, 'ent_concat')  ## no Clipping Gives MINE
  #    ent_mine_est_mi.append([np.mean(est_mi[-30:]), np.std(est_mi[-30:])])

  #    print("-----Trained SNR {} for method {}: Actual MI {:.2f}, Estimated MI {:.2f}".format(snr_db, 
  #                                                                                    'ent_mine',
  #                                                                                    true_mi,
 #                                                                                     np.mean(est_mi[-30:])))

      # pltax = np.linspace(1, opt_params['iterations'], opt_params['iterations'])
      # # plt.plot(pltax,np.array(MI_MINE)-np.array(MI_MINE1)-np.array(MI_MINE2), label='XY')
      # # plt.plot(pltax,MI_MINE1, label='X')
      # plt.plot(pltax, est_mi, label='approximation')
      # plt.show()
      # print('True MI is: ', true_mi)
      # print("Est last: ", est_mi[-1], "Est 20: ", np.mean(est_mi[-20:]), "Est 30: ", np.mean(est_mi[-30:]))

print(y_tiled)
print(batch_size)
print(xy_pairs)

s1=torch.tensor([[12,33,11,15],[11,43,1234,126]])
s2=torch.tensor([[17,0,90,98],[20,50,60,10]])
x_tiled=torch.stack([s1]*2,dim=0)
y_tiled=torch.stack([s2]*2,dim=1)
batch_size=2
xy_pairs = torch.reshape(torch.cat((x_tiled, y_tiled), dim=2), [
                                 batch_size * batch_size, -1])
print(xy_pairs)

# Commented out IPython magic to ensure Python compatibility.
from matplotlib.pyplot import figure
from plotly import __version__
# %matplotlib inline

import plotly.offline as pyo
import plotly.graph_objs as go
from plotly.offline import iplot

import cufflinks as cf
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot 
figure(num=None, figsize=(8, 6), dpi=150, facecolor='w', edgecolor='k')

def errorfill(x, y, yerr, color=None, alpha_fill=0.3, ax=None, label=None):
    ax = ax if ax is not None else plt.gca()
    if color is None:
        color = ax._get_lines.color_cycle.next()
    if np.isscalar(yerr) or len(yerr) == len(y):
        ymin = y - yerr
        ymax = y + yerr
    elif len(yerr) == 2:
        ymin, ymax = yerr
    ax.plot(x, y, color=color, label=label)
    ax.fill_between(x, ymax, ymin, color=color, alpha=alpha_fill)

plt.plot(all_snr_db, all_gt_mi, '--k', label='True MI')
errorfill(all_snr_db, np.array(smile_est_mi)[:,0], np.array(smile_est_mi)[:,1], color='r', label='SMILE Est.')
errorfill(all_snr_db, np.array(chisquare_est_mi)[:,0], np.array(chisquare_est_mi)[:,1], color='g', label='Chi_Square bound Est.')
errorfill(all_snr_db, np.array(ent_smile_est_mi)[:,0], np.array(ent_smile_est_mi)[:,1], color='m', label='Entropic SMILE Est.')
#errorfill(all_snr_db, np.array(ent_mine_est_mi)[:,0], np.array(ent_mine_est_mi)[:,1], color='b', label='Entropic MINE Est.')
plt.ylabel('Mutual Information (nats)')
plt.xlabel('SNR (dB)')
plt.legend()
plt.show